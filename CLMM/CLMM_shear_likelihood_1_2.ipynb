{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Shear Likelihood 1.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import gi\n",
    "\n",
    "gi.require_version('NumCosmo', '1.0')\n",
    "gi.require_version('NumCosmoMath', '1.0')\n",
    "from gi.repository import GObject\n",
    "from gi.repository import NumCosmo as Nc\n",
    "from gi.repository import NumCosmoMath as Ncm\n",
    "\n",
    "os.environ['CLMM_MODELING_BACKEND'] = 'nc'\n",
    "\n",
    "__name__ = \"NcContext\"\n",
    "\n",
    "Ncm.cfg_init ()\n",
    "Ncm.cfg_set_log_handler (lambda msg: sys.stdout.write (msg) and sys.stdout.flush ())\n",
    "\n",
    "try: import clmm\n",
    "except:\n",
    "    import notebook_install\n",
    "    notebook_install.install_clmm_pipeline(upgrade=False)\n",
    "    import clmm\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from astropy import units\n",
    "from numpy import random\n",
    "plt.rcParams['font.family']=['gothambook','gotham','gotham-book','serif']\n",
    "\n",
    "import clmm.dataops as da\n",
    "import clmm.galaxycluster as gc\n",
    "import clmm.theory as theory\n",
    "from clmm import Cosmology\n",
    "from clmm.support import mock_data as mock\n",
    "from clmm.utils import convert_units"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating mock data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(11)\n",
    "\n",
    "cosmo = Cosmology(H0 = 70.0, Omega_dm0 = 0.27 - 0.045, Omega_b0 = 0.045, Omega_k0 = 0.0)\n",
    "\n",
    "cluster_m     = 1.e15\n",
    "logm = np.log(cluster_m)/np.log(10)\n",
    "cluster_z     = 0.4\n",
    "concentration = 4\n",
    "ngals         = 10000\n",
    "Delta         = 200\n",
    "cluster_ra    = 20.0\n",
    "cluster_dec   = 90.0\n",
    "\n",
    "ideal_data   = mock.generate_galaxy_catalog(cluster_m, cluster_z, concentration, cosmo, 0.8, zsrc_min = cluster_z + 0.1, ngals=ngals, cluster_ra=cluster_ra, cluster_dec=cluster_dec)\n",
    "ideal_data_z = mock.generate_galaxy_catalog(cluster_m, cluster_z, concentration, cosmo,'chang13', zsrc_min = cluster_z + 0.1, ngals=ngals, cluster_ra=cluster_ra, cluster_dec=cluster_dec)\n",
    "noisy_data_z = mock.generate_galaxy_catalog(cluster_m, cluster_z, concentration, cosmo, 'chang13', zsrc_min = cluster_z + 0.1, shapenoise=0.05, photoz_sigma_unscaled=0.05, ngals=ngals, cluster_ra=cluster_ra, cluster_dec=cluster_dec)\n",
    "\n",
    "gc_ideal   = clmm.GalaxyCluster(\"CL_ideal\", cluster_ra, cluster_dec, cluster_z, ideal_data)\n",
    "gc_ideal_z = clmm.GalaxyCluster(\"CL_ideal_z\", cluster_ra, cluster_dec, cluster_z, ideal_data_z)\n",
    "gc_noisy_z = clmm.GalaxyCluster(\"CL_noisy_z\", cluster_ra, cluster_dec, cluster_z, noisy_data_z)\n",
    "\n",
    "# For some reason galaxy catalogs access information very deifferently from galaxy clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating observables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta_ideal, gt_ideal, gx_ideal       = gc_ideal.compute_tangential_and_cross_components(geometry=\"flat\")\n",
    "theta_ideal_z, gt_ideal_z, gx_ideal_z = gc_ideal_z.compute_tangential_and_cross_components(geometry=\"flat\")\n",
    "theta_noisy_z, gt_noisy_z, gx_noisy_z = gc_noisy_z.compute_tangential_and_cross_components(geometry=\"flat\")\n",
    "\n",
    "# Convert from theta to radius\n",
    "radius_ideal, radius_ideal_z, radius_noisy_z = [convert_units(gc.galcat['theta'], 'radians', 'Mpc', redshift=cluster_z, cosmo=cosmo) for gc in [gc_ideal, gc_ideal_z, gc_noisy_z]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Binning the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/caiooliveira/.local/lib/python3.10/site-packages/clmm-1.1.10-py3.10.egg/clmm/utils.py:176: RuntimeWarning: invalid value encountered in sqrt\n"
     ]
    }
   ],
   "source": [
    "bin_edges = da.make_bins(0.7, 4, 15, method='evenlog10width')\n",
    "\n",
    "profile_ideal   = gc_ideal.make_radial_profile(\"Mpc\", bins=bin_edges,cosmo=cosmo, gal_ids_in_bins=True)\n",
    "profile_ideal_z = gc_ideal_z.make_radial_profile(\"Mpc\", bins=bin_edges,cosmo=cosmo, gal_ids_in_bins=True)\n",
    "profile_noisy_z = gc_noisy_z.make_radial_profile(\"Mpc\", bins=bin_edges,cosmo=cosmo, gal_ids_in_bins=True)\n",
    "\n",
    "profiles = [profile_ideal, profile_ideal_z, profile_noisy_z]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating Likelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The measured shear $g^i_t$ is given by \n",
    "\n",
    "$$ g^i_t = g^{I,i}_t + g^{L,i}_t $$\n",
    "\n",
    "where $g^{I,i}_t$ is the intrisic shear and $g^{L,i}_t$ is the lensing shear. Assuming that the intrinsic shear is drawn from a normal distribution with average 0 and variance $\\sigma_{g^I_t}^2$, the probability of measuring $g^i_t$, given a certain $r$, $z$ and other cosmological parameters $\\vec{\\theta_c}$ is given by\n",
    "\n",
    "$$ P(g^i_t | r, z, \\vec{\\theta_c}) = \\frac{1}{\\sqrt{2\\pi}\\sigma_{g^I_t}} \\exp\\left[\\frac{-(g^i_t - g^{L,i}_t(r,z,\\vec{\\theta_c}))^2}{2\\sigma_{g^I_t}^2}\\right] $$\n",
    "Then, to find the likelihood of the set of all $\\{g^i_t\\}_i^{N}$ measured, we perform the following integral for all $g^i_t$:\n",
    "\n",
    "$$ P(g^i_t | \\vec{\\theta_c}) = \\int_0^{r_{max}} \\int_{z_{cluster}}^{\\infty} \\mathcal{d}r \\mathcal{d}z P(g^i_t | r, z, \\vec{\\theta_c}) P(r) P(z) $$\n",
    "\n",
    "where $P(r)$ and $P(z)$ are the distributions for $r$ and $z$.\n",
    "\n",
    "This is a computationally expensive process though. To avoid it, we bin the data. We then have to perform the same integrals, but for each bin and each shear $g^i_t$ in the bin. At this moment we'll choose to not calculate them, and instead use an estimator for the probability\n",
    "\n",
    "$$ \\tilde{P}(g^i_t | \\vec{\\theta_c}) = \\frac{1}{N_i} \\sum_j^{N_i} P(g^i_t | r_j, z_j, \\vec{\\theta_c}) $$\n",
    "\n",
    "where $N_i$ is the number of galaxies in the bin. The bin likelihood is then\n",
    "\n",
    "$$ \\mathcal{L}_i = \\prod_j^{N_i} \\tilde{P}(g^j_t | \\vec{\\theta_c}) $$\n",
    "\n",
    "and the total likelihood is \n",
    "\n",
    "$$ \\mathcal{L} = \\prod_i^M \\mathcal{L}_i = \\prod_i^M \\prod_j^{N_i} \\tilde{P}(g^j_t | \\vec{\\theta_c}) =  \\prod_i^M \\prod_j^{N_i} \\left[ \\frac{1}{N_i} \\sum_k^{N_i} P(g^j_t | r_k, z_k, \\vec{\\theta_c}) \\right]  $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Probability of measuring gt_obs given a certain r, z, cosmo, etc.\n",
    "def shear_probability(r, gal_z, cluster_z, gt_obs, sigma, cosmo, m=1.e15, concentration=4, delta=200, model='nfw'):\n",
    "\n",
    "    gt_th = clmm.compute_reduced_tangential_shear(r, m, concentration, cluster_z, gal_z, cosmo, delta_mdef=200, halo_profile_model=model)\n",
    "\n",
    "    return np.exp(-np.power(np.subtract(gt_obs, gt_th), 2)/2/sigma**2)/np.sqrt(2*np.pi)/sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shear_log_likelihood(radius, gt, gc_data, cosmo, sigma=1, m=1.e15, concentration=4, delta=200, model='nfw'):\n",
    "\n",
    "    n_bins = len(gc_data.profile)\n",
    "    cluster_z = gc_data.z\n",
    "    likelihood = 0\n",
    "\n",
    "    for i in range(n_bins):\n",
    "\n",
    "        gal_list = gc_data.profile['gal_id'][i]\n",
    "        z_list = gc_data.galcat['z'][gal_list]\n",
    "        r_list = [radius[gal_id] for gal_id in gal_list]\n",
    "        gt_prob_bin = 0\n",
    "\n",
    "        for gal in gal_list:\n",
    "\n",
    "            gt_obs_gal = gt[gal]\n",
    "            gt_prob = np.mean(shear_probability(r_list, z_list, cluster_z, gt_obs_gal, sigma, cosmo, m, concentration, delta, model))\n",
    "            gt_prob_bin += np.log(gt_prob)\n",
    "\n",
    "        likelihood += gt_prob_bin\n",
    "\n",
    "    return -2*likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14002.640036901772"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shear_log_likelihood(radius_ideal_z, gt_ideal_z, gc_ideal_z, cosmo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
